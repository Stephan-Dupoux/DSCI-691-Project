{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mining ADE from English Tweets\n",
    "### Task 1a - Classification\n",
    "*GOAL: Create a baseline model for classification*  \n",
    "*CLASSIFY IF A DRUG EVENT IS PRESENT IN A TWEET - (`ADE`/`NoADE`)*\n",
    "### Baseline Models:  \n",
    "1. Logistic Regression\n",
    "2. TF-IDF encoding, SVM classifier\n",
    "3. GloVe embedding transformation, SVM classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17120, 2)\n",
      "(17385, 2)\n"
     ]
    }
   ],
   "source": [
    "tweets = pd.read_csv('../data/training/tweets.tsv', sep='\\t', header=None, names=['tweet_id', 'tweet'])\n",
    "classes = pd.read_csv('../data/training/class.tsv', sep='\\t', header=0)\n",
    "\n",
    "print(tweets.shape)\n",
    "print(classes.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre-processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are more classses (n=17,385) than tweets(n=17,120)\n",
    "data = pd.merge(tweets, classes, how='left')\n",
    "\n",
    "# 1.1. Remove '@USER' and any proceeding '_' from tweet variable in dataframe\n",
    "# data = data.replace(r'@\\w+\\s', '', regex=True)\n",
    "\n",
    "# # remove emojis\n",
    "# data = data.encode('ascii', errors='ignore').decode('utf8').strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(tweet, remove_stopwords = True):\n",
    "    # clean\n",
    "    tweet = tweet.lower().strip()\n",
    "    # remove user mentions\n",
    "    tweet = re.sub(r'@\\w+\\s', '', tweet)\n",
    "    # remove any emoji\n",
    "    tweet = tweet.encode('ascii', errors='ignore').decode('utf8').strip()\n",
    "    # stopwords\n",
    "    if remove_stopwords:\n",
    "        tweet = ' '.join([word for word in tweet.split() if word not in stop_words])\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply clean_tweets to the tweet column\n",
    "data['tweet'] = data['tweet'].apply(clean_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    tweet_id  \\\n",
      "0  SMM4H2022qLUQBRHYD1Z3P0Bw   \n",
      "1  SMM4H2022Y7dIrCNzlZ6OHWmY   \n",
      "2  SMM4H2022YDKxa7o2NC3erEdl   \n",
      "3  SMM4H2022qovo6eqwcAGzSMC1   \n",
      "4  SMM4H2022Bz5GaKU5KUEK9qDC   \n",
      "\n",
      "                                               tweet  label  \n",
      "0  knowledge power!levaquin antibiotic interacts ...      0  \n",
      "1  methylpred, glatiramer acetate, interferon alp...      0  \n",
      "2                             // .... cymbalta help.      0  \n",
      "3        think imodium works . full stop. #allegedly      0  \n",
      "4  meanwhile, get flavorless gelatin lamotrigine....      0  \n"
     ]
    }
   ],
   "source": [
    "# other checks\n",
    "# are there duplicates?\n",
    "np.sum(data.duplicated()) \n",
    "# NO!\n",
    "# are there missing values?\n",
    "data.isnull().sum()\n",
    "# no missing values!\n",
    "\n",
    "# convert label to binary\n",
    "data = data.replace(['NoADE', 'ADE'], [0, 1])\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train index: [ 3360  9091  6988 ...   910 12559   444] Test index: [ 5122  1796   298 ...  1025  2309 14625]\n"
     ]
    }
   ],
   "source": [
    "# use stratified sampling to balance the classes\n",
    "strat_split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=691)\n",
    "X = data['tweet'].to_numpy()\n",
    "y = data['label'].to_numpy()\n",
    "\n",
    "for train_index, test_index in strat_split.split(X, y):\n",
    "    print(f\"Train index: {train_index}\", f\"Test index: {test_index}\")\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EDA of splits**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    15909\n",
       "1     1211\n",
       "dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. EDA\n",
    "# from pandas_profiling import ProfileReport\n",
    "# profile_train = pandas_profiling.ProfileReport(train, title=\"Pandas Profiling Report (Train)\")\n",
    "# profile_train.to_file(\"DSCI691-GRP-PICKLE_RICK/Task_1/subtask_1a/profile_train.html\")\n",
    "# check for class imbalance\n",
    "data.groupby('label').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- There is a class imbalance in the outcome variable \"class\"\n",
    "- Only 7.2% of the tweets are labeled as \"NoADE\"\n",
    "- see report for more details\n",
    "- suggestion from jake (05/26): keep data as is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# visualize the distribution of y_train data\n",
    "import matplotlib.pyplot as plt\n",
    "ys = pd.Series(y_train)\n",
    "ys.value_counts().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test data\n",
    "ys2 = pd.Series(y_test)\n",
    "ys2.value_counts().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text representation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13696, 19552)\n",
      "(3424, 19552)\n"
     ]
    }
   ],
   "source": [
    "# convert tweets to matrix of word counts and remove stop words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "countvec = CountVectorizer(stop_words='english')\n",
    "\n",
    "# normalise count matrix to decrease the effect of word frequencies\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf = TfidfTransformer()\n",
    "\n",
    "# vectorize and transform train and test data\n",
    "train_transformed = tfidf.fit_transform(countvec.fit_transform(X_train))\n",
    "test_transformed = tfidf.transform(countvec.transform(X_test))\n",
    "\n",
    "print(train_transformed.shape)\n",
    "print(test_transformed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2908,  274],\n",
       "       [  73,  169]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# course notes uses the 'liblinear' solver however sklearn uses the 'lbfgs' solver as default\n",
    "log_reg = LogisticRegression(solver='lbfgs', random_state=691, class_weight='balanced')\n",
    "\n",
    "# fit\n",
    "log_reg.fit(train_transformed, y_train)\n",
    "y_pred = log_reg.predict(test_transformed)\n",
    "\n",
    "# confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "mat = confusion_matrix(y_test, y_pred)\n",
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:\n",
      "AUC: 0.8061188711294419\n",
      "Precision: 0.38\n",
      "Recall: 0.70\n",
      "F1 Score: 0.49\n"
     ]
    }
   ],
   "source": [
    "# print results\n",
    "print(f\"Logistic Regression:\")\n",
    "print(f\"AUC: {roc_auc_score(y_test, y_pred)}\")  #0.806\n",
    "print(f\"Precision: {precision_recall_fscore_support(y_test, y_pred, average='binary', pos_label=1)[0]:.2f}\") # 0.38\n",
    "print(f\"Recall: {precision_recall_fscore_support(y_test, y_pred, average='binary', pos_label=1)[1]:.2f}\") # 0.70\n",
    "print(f\"F1 Score: {precision_recall_fscore_support(y_test, y_pred, average='binary', pos_label=1)[2]:.2f}\") # 0.49\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard SVM classifier with TF-IDF features\n",
    "# linear kernel\n",
    "sv_m = SVC(kernel='linear', class_weight='balanced', random_state=691)\n",
    "# fit\n",
    "sv_m.fit(train_transformed, y_train)\n",
    "y_pred_sv = sv_m.predict(test_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM:\n",
      "AUC: 0.7717507051545107\n",
      "Precision: 0.47\n",
      "Recall: 0.60\n",
      "F1 Score: 0.52\n"
     ]
    }
   ],
   "source": [
    "# print results\n",
    "print(f\"SVM:\")\n",
    "print(f\"AUC: {roc_auc_score(y_test, y_pred_sv)}\")\n",
    "print(f\"Precision: {precision_recall_fscore_support(y_test, y_pred_sv, average='binary', pos_label=1)[0]:.2f}\")\n",
    "print(f\"Recall: {precision_recall_fscore_support(y_test, y_pred_sv, average='binary', pos_label=1)[1]:.2f}\")\n",
    "print(f\"F1 Score: {precision_recall_fscore_support(y_test, y_pred_sv, average='binary', pos_label=1)[2]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM with grid search\n",
    "# define the parameter grid\n",
    "param_grid = {'kernel':['linear'], 'C' : [0.1, 1, 10, 100], 'gamma': [1, .01, 'auto', 'scale']}\n",
    "estimator = SVC(class_weight='balanced', random_state=691)\n",
    "clf = GridSearchCV(estimator, param_grid, cv=5)\n",
    "# fit\n",
    "clf.fit(train_transformed, y_train)\n",
    "y_pred_gs = clf.predict(test_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3068,  114],\n",
       "       [ 134,  108]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred_gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM with grid search + 5 cv:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96      3182\n",
      "           1       0.49      0.45      0.47       242\n",
      "\n",
      "    accuracy                           0.93      3424\n",
      "   macro avg       0.72      0.71      0.71      3424\n",
      "weighted avg       0.92      0.93      0.93      3424\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"SVM with grid search + 5 cv:\")\n",
    "print(metrics.classification_report(y_test, y_pred_gs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. SVM with pre-trained GloVe\n",
    "Transfer learning  \n",
    "Source for [GloVe Twitter](https://nlp.stanford.edu/projects/glove/)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GloVe Twitter embeddings - 100-dimensional embeddings for each word\n",
    "# 1. convert GloVe format to word2vec format\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "# note: fill too large for GH\n",
    "# glove_input_file = 'DSCI691-GRP-PICKLE_RICK/Project/glove.twitter.27B.100d.txt'\n",
    "# word2vec_output_file = 'DSCI691-GRP-PICKLE_RICK/Project/glove.twitter.27B.100d.txt.word2vec'\n",
    "# glove2word2vec(glove_input_file, word2vec_output_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('drugs', 0.8083562254905701),\n",
       " ('marijuana', 0.7322571873664856),\n",
       " ('cocaine', 0.7105314135551453),\n",
       " ('addiction', 0.6785678863525391),\n",
       " ('dealers', 0.6764513850212097),\n",
       " ('heroin', 0.6691657900810242),\n",
       " ('meth', 0.6684464812278748),\n",
       " ('dealer', 0.6547229886054993),\n",
       " ('pharmacy', 0.6478521823883057),\n",
       " ('illegal', 0.6464824080467224)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. build a baseline word2vec model\n",
    "# load GloVe word vectors\n",
    "filepath = '../../../Drexel/spring_22/DSCI691/DSCI691-GRP-PICKLE_RICK/Project/glove.twitter.27B/glove.twitter.27B.100d.txt.word2vec'\n",
    "from gensim.models import KeyedVectors\n",
    "glove_vec = KeyedVectors.load_word2vec_format(filepath, binary=False)   \n",
    "glove_vec.most_similar('drug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['knowledge', 'is', 'powerlevaquin', 'antibiotic', 'interacts', 'with', 'the', 'medication', 'seroquel', 'and', 'can', 'cause', 'heart', 'arrthymiaslearnt', 'something', 'new', 'today', 'yes'], ['methylpred', 'glatiramer', 'acetate', 'interferon', 'alpha', 'n', 'beta', 'baclofen', 'natalizumabwhat', 'am', 'i', 'thinking'], ['cymbalta', 'can', 'help'], ['I', 'dont', 'think', 'Imodium', 'works', 'Full', 'stop', 'allegedly'], ['Meanwhile', 'all', 'I', 'get', 'is', 'flavorless', 'gelatin', 'lamotrigine', 'Fuck', 'This']]\n"
     ]
    }
   ],
   "source": [
    "# create baseline word2vec model with tweet data\n",
    "# input: list of tokenized tweets\n",
    "tweets_ls = []\n",
    "for tweet in data['tweet']:\n",
    "    tweets_ls.append(tweet.split())\n",
    "    \n",
    "print(tweets_ls[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17120"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply word2vec transformation from GloVe pre-trained word embedding\n",
    "import gensim.models as gm\n",
    "# `workers` is the number of cores to use and does not work without Cython\n",
    "import Cython\n",
    "base_model = gm.Word2Vec(tweets_ls, vector_size=200, min_count=1, workers=4)\n",
    "# ran in 1.4 seconds\n",
    "base_model.build_vocab(tweets_ls)\n",
    "total = base_model.corpus_count\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('of', 0.9951425194740295),\n",
       " ('in', 0.995005190372467),\n",
       " ('at', 0.9937447905540466),\n",
       " ('injection', 0.9935187101364136),\n",
       " ('returns', 0.9928048253059387),\n",
       " ('diazepam', 0.9920201897621155),\n",
       " ('18885205202', 0.9917005300521851),\n",
       " ('next', 0.991690993309021),\n",
       " ('shape', 0.9910983443260193),\n",
       " ('Reuters', 0.9903740882873535)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrain base_model with GloVe vocaublary and starting weights\n",
    "base_model.build_vocab([glove_vec.index_to_key], update=True)\n",
    "# train on tweets\n",
    "base_model.train(tweets_ls, total_examples=total, epochs=base_model.epochs)\n",
    "# set of word vectors with glove weights and trained on tweets\n",
    "base_model_wv = base_model.wv\n",
    "base_model_wv.most_similar('drug')\n",
    "\n",
    "# this looks worse than the glove model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to transform tweets to word2vec vectors\n",
    "# accounts for dimensionality of vectors - if word not in base_model_wv, use 0 vector\n",
    "# uses the mean of all word vectors in tweet\n",
    "def tweet_to_wv(tweets, theModel):\n",
    "    # model = theModel\n",
    "    tweet_wv = []\n",
    "    for tweet in tweets:\n",
    "        tweet_vec = np.zeros(200)\n",
    "        for word in tweet:\n",
    "            if word in theModel.index_to_key:\n",
    "                tweet_vec += theModel[word]\n",
    "            else:\n",
    "                tweet_vec += np.zeros(200)\n",
    "        tweet_vec /= len(tweet)\n",
    "        tweet_wv.append(tweet_vec)\n",
    "    return tweet_wv\n",
    "\n",
    "# transform train and test data\n",
    "train_wv = tweet_to_wv(X_train, base_model_wv)\n",
    "test_wv = tweet_to_wv(X_test, base_model_wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM with model trained on tweets\n",
    "# linear kernel\n",
    "svm_wv = SVC(kernel='linear', class_weight='balanced', random_state=691)\n",
    "# fit\n",
    "svm_wv.fit(train_wv, y_train)\n",
    "y_pred_wv = svm_wv.predict(test_wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "import pickle\n",
    "# filename = '../data/svm_wv.sav'\n",
    "# pickle.dump(svm_wv, open(filename, 'wb'))\n",
    "# open model\n",
    "pickle.load(open('../data/svm_wv.sav', 'rb'))\n",
    "# print metrics\n",
    "print(f\"SVM with word2vec features:\")\n",
    "print(metrics.classification_report(y_test, y_pred_wv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using GloVe v.2**  \n",
    "Just expand `base_model` vocab with glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try without training on tweets\n",
    "base_model2 = gm.Word2Vec(tweets_ls, vector_size=200, min_count=1, workers=4)\n",
    "# ran in 1.4 seconds\n",
    "base_model2.build_vocab(tweets_ls)\n",
    "base_model2.build_vocab([glove_vec.index_to_key], update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('thought', 0.988355278968811),\n",
       " ('video', 0.9881336688995361),\n",
       " ('biib', 0.9877603054046631),\n",
       " ('hands', 0.98773592710495),\n",
       " ('Birth', 0.987488865852356),\n",
       " ('other', 0.9874578714370728),\n",
       " ('lithium', 0.987389326095581),\n",
       " ('card', 0.9873044490814209),\n",
       " ('tenofovir', 0.9871808886528015),\n",
       " ('easy', 0.9870908260345459)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model_wv2 = base_model2.wv\n",
    "base_model_wv2.most_similar('drug')\n",
    "# still bad :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform train and test data\n",
    "train_wv2 = tweet_to_wv(X_train, base_model_wv2)\n",
    "test_wv2 = tweet_to_wv(X_test, base_model_wv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM with model trained on tweets\n",
    "# linear kernel\n",
    "svm_wv2 = SVC(kernel='linear', class_weight='balanced', random_state=691)\n",
    "# fit\n",
    "svm_wv2.fit(train_wv2, y_train)\n",
    "y_pred_wv2 = svm_wv.predict(test_wv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"SVM with word2vec features but no training on tweets:\")\n",
    "print(metrics.classification_report(y_test, y_pred_wv2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using GloVe v.3**  \n",
    "Foregoing `base_model` all together and just using glove_vec out of the box to transform tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function only looks at glove vectors\n",
    "\n",
    "# transform train and test data ~ 1.5hr\n",
    "train_glove = tweet_to_wv(X_train, glove_vec)\n",
    "test_glove = tweet_to_wv(X_test, glove_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM with model trained on tweets\n",
    "# linear kernel\n",
    "svm_glove = SVC(kernel='linear', class_weight='balanced', random_state=691)\n",
    "# fit\n",
    "svm_glove.fit(train_glove, y_train)\n",
    "y_pred_glove = svm_wv.predict(test_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "# filename = '../data/svm_wv.sav'\n",
    "# pickle.dump(svm_wv, open(filename, 'wb'))\n",
    "# open model\n",
    "# pickle.load(open('../data/svm_wv.sav', 'rb'))\n",
    "# print metrics\n",
    "print(f\"SVM with glove features:\")\n",
    "print(metrics.classification_report(y_test, y_pred_glove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM with model using just glove vectors\n",
    "# linear kernel\n",
    "svm_wv = SVC(kernel='linear', class_weight='balanced', random_state=691)\n",
    "# fit\n",
    "svm_wv.fit(train_wv2, y_train)\n",
    "y_pred_wv2 = svm_wv.predict(test_wv2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*SVM classifier with rbf kernel*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_wv_rbf = SVC(kernel='rbf', class_weight='balanced', random_state=691)\n",
    "# fit\n",
    "svm_wv_rbf.fit(train_wv, y_train)\n",
    "y_pred_wv_rbf = svm_wv_rbf.predict(test_wv)\n",
    "# save model\n",
    "filename = 'DSCI691-GRP-PICKLE_RICK/Project/svm_wv_rbf.sav'\n",
    "pickle.dump(svm_wv_rbf, open(filename, 'wb'))\n",
    "# print metrics\n",
    "print(f\"SVM with word2vec features and rbf kernel:\")\n",
    "print(metrics.classification_report(y_test, y_pred_wv_rbf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*SVM classifier with rbf kernel + grid search*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'C' : [0.1, 1, 10],\n",
    "    'gamma' : [1, 'auto', 'scale']\n",
    "}\n",
    "svm_wv_rbf2 = GridSearchCV(SVC(kernel='rbf', class_weight='balanced', random_state=691), parameters, cv=5)\n",
    "# fit\n",
    "svm_wv_rbf2.fit(train_wv, y_train)\n",
    "y_pred_wv_rbf2 = svm_wv_rbf2.predict(test_wv)\n",
    "# save model\n",
    "filename = 'DSCI691-GRP-PICKLE_RICK/Project/svm_wv_rbf2.sav'\n",
    "pickle.dump(svm_wv_rbf2, open(filename, 'wb'))\n",
    "# print metrics\n",
    "print(f\"SVM with word2vec features and rbf kernel and gridsearch:\")\n",
    "print(metrics.classification_report(y_test, y_pred_wv_rbf2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'C' : [0.1, 1, 10],\n",
    "    'gamma' : [1, 'auto', 'scale']\n",
    "}\n",
    "svm_gs = GridSearchCV(SVC(kernel='linear', class_weight='balanced', random_state=691), parameters, cv=5)\n",
    "# fit\n",
    "svm_gs.fit(train_wv2, y_train)\n",
    "y_pred_gs = svm_gs.predict(test_wv2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM with glove features and linear kernel and gridsearch:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.66      0.78      3182\n",
      "           1       0.14      0.72      0.23       242\n",
      "\n",
      "    accuracy                           0.66      3424\n",
      "   macro avg       0.55      0.69      0.51      3424\n",
      "weighted avg       0.91      0.66      0.74      3424\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "# filename = 'DSCI691-GRP-PICKLE_RICK/Project/svm_wv_rbf2.sav'\n",
    "# pickle.dump(svm_wv_rbf2, open(filename, 'wb'))\n",
    "# print metrics\n",
    "print(f\"SVM with glove features and linear kernel and gridsearch:\")\n",
    "print(metrics.classification_report(y_test, y_pred_gs))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
