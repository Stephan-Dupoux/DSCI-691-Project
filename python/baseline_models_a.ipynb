{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mining ADE from English Tweets\n",
    "### Task 1a - Classification\n",
    "*GOAL: Create a baseline model for classification*  \n",
    "*CLASSIFY IF A DRUG EVENT IS PRESENT IN A TWEET - (`ADE`/`NoADE`)*\n",
    "### Baseline Models:  \n",
    "1. Logistic Regression\n",
    "2. TF-IDF encoding, SVM classifier\n",
    "3. GloVe embedding transformation, SVM classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import pandas_profiling\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "from collections import Counter\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('DSCI691-GRP-PICKLE_RICK/Task_1/subtask_1a/data/training/tweets.tsv', sep='\\t', header=None,\n",
    "                     names=['tweet_id', 'tweet'])\n",
    "classes = pd.read_csv('DSCI691-GRP-PICKLE_RICK/Task_1/subtask_1a/data/training/class.tsv', sep='\\t', header=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre-processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are more classses (n=17,385) than tweets(n=17,120)\n",
    "data = pd.merge(tweets, classes, how='left')\n",
    "\n",
    "# 1.1. Remove '@USER' and any proceeding '_' from tweet variable in dataframe\n",
    "data = data.replace(r'@\\w+', '', regex=True)\n",
    "\n",
    "# remove any emoji from the tweet\n",
    "data = data.replace(r'[^\\w\\s]', '', regex=True)\n",
    "\n",
    "# are there duplicates?\n",
    "np.sum(data.duplicated()) \n",
    "# NO!\n",
    "# are there missing values?\n",
    "data.isnull().sum()\n",
    "# no missing values!\n",
    "\n",
    "# convert label to binary\n",
    "data = data.replace(['NoADE', 'ADE'], [0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use stratified sampling to balance the classes\n",
    "strat_split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=691)\n",
    "X = data['tweet'].to_numpy()\n",
    "y = data['label'].to_numpy()\n",
    "\n",
    "for train_index, test_index in strat_split.split(X, y):\n",
    "    print(f\"Train index: {train_index}\", f\"Test index: {test_index}\")\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EDA of splits**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. EDA\n",
    "# from pandas_profiling import ProfileReport\n",
    "# profile_train = pandas_profiling.ProfileReport(train, title=\"Pandas Profiling Report (Train)\")\n",
    "# profile_train.to_file(\"DSCI691-GRP-PICKLE_RICK/Task_1/subtask_1a/profile_train.html\")\n",
    "# check for class imbalance\n",
    "data.groupby('label').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- There is a class imbalance in the outcome variable \"class\"\n",
    "- Only 7.2% of the tweets are labeled as \"NoADE\"\n",
    "- see report for more details\n",
    "- suggestion from jake (05/26): keep data as is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# visualize the distribution of y_train data\n",
    "import matplotlib.pyplot as plt\n",
    "ys = pd.Series(y_train)\n",
    "ys.value_counts().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data\n",
    "ys2 = pd.Series(y_test)\n",
    "ys2.value_counts().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text representation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert tweets to matrix of word counts and remove stop words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "countvec = CountVectorizer(stop_words='english')\n",
    "\n",
    "# normalise count matrix to decrease the effect of word frequencies\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf = TfidfTransformer()\n",
    "\n",
    "# vectorize and transform train and test data\n",
    "train_transformed = tfidf.fit_transform(countvec.fit_transform(X_train))\n",
    "test_transformed = tfidf.transform(countvec.transform(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# course notes uses the 'liblinear' solver however sklearn uses the 'lbfgs' solver as default\n",
    "log_reg = LogisticRegression(solver='lbfgs', random_state=691, class_weight='balanced')\n",
    "\n",
    "# fit\n",
    "log_reg.fit(train_transformed, y_train)\n",
    "y_pred = log_reg.predict(test_transformed)\n",
    "\n",
    "# confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "mat = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print results\n",
    "print(f\"Logistic Regression:\")\n",
    "print(f\"AUC: {roc_auc_score(y_test, y_pred)}\")  #0.797\n",
    "print(f\"Precision: {precision_recall_fscore_support(y_test, y_pred, average='binary', pos_label=1)[0]:.2f}\") # 0.37\n",
    "print(f\"Recall: {precision_recall_fscore_support(y_test, y_pred, average='binary', pos_label=1)[1]:.2f}\") # 0.68\n",
    "print(f\"F1 Score: {precision_recall_fscore_support(y_test, y_pred, average='binary', pos_label=1)[2]:.2f}\") # 0.48\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard SVM classifier with TF-IDF features\n",
    "# linear kernel\n",
    "sv_m = SVC(kernel='linear', class_weight='balanced', random_state=691)\n",
    "# fit\n",
    "sv_m.fit(train_transformed, y_train)\n",
    "y_pred_sv = sv_m.predict(test_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print results\n",
    "print(f\"SVM:\")\n",
    "print(f\"AUC: {roc_auc_score(y_test, y_pred_sv)}\")\n",
    "print(f\"Precision: {precision_recall_fscore_support(y_test, y_pred_sv, average='binary', pos_label=1)[0]:.2f}\")\n",
    "print(f\"Recall: {precision_recall_fscore_support(y_test, y_pred_sv, average='binary', pos_label=1)[1]:.2f}\")\n",
    "print(f\"F1 Score: {precision_recall_fscore_support(y_test, y_pred_sv, average='binary', pos_label=1)[2]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. SVM with pre-trained GloVe\n",
    "Transfer learning  \n",
    "Source for [GloVe Twitter](https://nlp.stanford.edu/projects/glove/)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GloVe Twitter embeddings - 100-dimensional embeddings for each word\n",
    "# 1. convert GloVe format to word2vec format\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove_input_file = 'DSCI691-GRP-PICKLE_RICK/Project/glove.twitter.27B.100d.txt'\n",
    "word2vec_output_file = 'DSCI691-GRP-PICKLE_RICK/Project/glove.twitter.27B.100d.txt.word2vec'\n",
    "glove2word2vec(glove_input_file, word2vec_output_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. build a baseline word2vec model\n",
    "from gensim.models import KeyedVectors\n",
    "glove_vec = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)   \n",
    "glove_vec.most_similar('drug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create baseline word2vec model with tweet data\n",
    "# input: list of tokenized tweets\n",
    "tweets_ls = []\n",
    "for tweet in data['tweet']:\n",
    "    tweets_ls.append(tweet.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply word2vec transformation from GloVe pre-trained word embedding\n",
    "import gensim.models as gm\n",
    "# `workers` is the number of cores to use and does not work without Cython\n",
    "import Cython\n",
    "base_model = gm.Word2Vec(tweets_ls, vector_size=200, min_count=1, workers=4)\n",
    "# ran in 1.4 seconds\n",
    "base_model.build_vocab(tweets_ls)\n",
    "total = base_model.corpus_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrain base_model with GloVe vocaublary and starting weights\n",
    "base_model.build_vocab([glove_vec.index_to_key], update=True)\n",
    "# train on tweets\n",
    "base_model.train(tweets_ls, total_examples=total, epochs=base_model.epochs)\n",
    "# set of word vectors with glove weights and trained on tweets\n",
    "base_model_wv = base_model.wv\n",
    "base_model_wv.most_similar('drug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to transform tweets to word2vec vectors\n",
    "# accounts for dimensionality of vectors - if word not in base_model_wv, use 0 vector\n",
    "# uses the mean of all word vectors in tweet\n",
    "def tweet_to_wv(tweets):\n",
    "    tweet_wv = []\n",
    "    for tweet in tweets:\n",
    "        tweet_vec = np.zeros(200)\n",
    "        for word in tweet:\n",
    "            if word in base_model_wv.index_to_key:\n",
    "                tweet_vec += base_model_wv[word]\n",
    "            else:\n",
    "                tweet_vec += np.zeros(200)\n",
    "        tweet_vec /= len(tweet)\n",
    "        tweet_wv.append(tweet_vec)\n",
    "    return tweet_wv\n",
    "\n",
    "# transform train and test data\n",
    "train_wv = tweet_to_wv(X_train)\n",
    "test_wv = tweet_to_wv(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Build classifier with W2V features*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear kernel\n",
    "svm_wv = SVC(kernel='linear', class_weight='balanced', random_state=691)\n",
    "# fit ~ 1hr\n",
    "svm_wv.fit(train_wv, y_train)\n",
    "y_pred_wv = svm_wv.predict(test_wv)\n",
    "# save model\n",
    "import pickle\n",
    "filename = 'DSCI691-GRP-PICKLE_RICK/Project/svm_wv.sav'\n",
    "pickle.dump(svm_wv, open(filename, 'wb'))\n",
    "\n",
    "# print metrics\n",
    "from sklearn import metrics\n",
    "print(f\"SVM with word2vec features:\")\n",
    "print(metrics.classification_report(y_test, y_pred_wv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*SVM classifier with rbf kernel*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_wv_rbf = SVC(kernel='rbf', class_weight='balanced', random_state=691)\n",
    "# fit\n",
    "svm_wv_rbf.fit(train_wv, y_train)\n",
    "y_pred_wv_rbf = svm_wv_rbf.predict(test_wv)\n",
    "# save model\n",
    "filename = 'DSCI691-GRP-PICKLE_RICK/Project/svm_wv_rbf.sav'\n",
    "pickle.dump(svm_wv_rbf, open(filename, 'wb'))\n",
    "# print metrics\n",
    "print(f\"SVM with word2vec features and rbf kernel:\")\n",
    "print(metrics.classification_report(y_test, y_pred_wv_rbf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*SVM classifier with rbf kernel + grid search*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {\n",
    "    'C' : [0.1, 1, 10],\n",
    "    'gamma' : [1, 'auto', 'scale']\n",
    "}\n",
    "svm_wv_rbf2 = GridSearchCV(SVC(kernel='rbf', class_weight='balanced', random_state=691), parameters, cv=5)\n",
    "# fit\n",
    "svm_wv_rbf2.fit(train_wv, y_train)\n",
    "y_pred_wv_rbf2 = svm_wv_rbf2.predict(test_wv)\n",
    "# save model\n",
    "filename = 'DSCI691-GRP-PICKLE_RICK/Project/svm_wv_rbf2.sav'\n",
    "pickle.dump(svm_wv_rbf2, open(filename, 'wb'))\n",
    "# print metrics\n",
    "print(f\"SVM with word2vec features and rbf kernel and gridsearch:\")\n",
    "print(metrics.classification_report(y_test, y_pred_wv_rbf2))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
